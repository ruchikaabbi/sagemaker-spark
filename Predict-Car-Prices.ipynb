{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook Instance Backed by Spark on EMR using Apache Livy \n",
    "- Before you begin, Follow the steps here to create an EMR cluster: https://github.com/ruchikaabbi/sagemaker-spark\n",
    "- Follow the steps here to connect SageMaker Notebook to Spark Livy EMR Cluster: https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/. The instructions cover the following:\n",
    "    - Make sure this SageMaker Notebook was launched with a security group and you have added access on port 8998 of EMR Cluster Master node to Sagemaker Notebook Security Group.\n",
    "    - Get the Private IP of EMR (with Livy) Master Node\n",
    "    - Open Terminal and make sure you can conenct to EMR using cmd: curl <EMR Master Private IP>:8998/sessions\n",
    "    - Modify ~/.sparkmagic/config.json from the terminal using steps in the blogpost. \n",
    "    - Come back to this notebook and restart Kernel\n",
    "- Change your s3_model_bucket and spark_model_location \n",
    "- Use US-West-2 (Oregon Region for the lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "## Do not update source input file path ##\n",
    "input_file_path = \"s3://ruchika-wibd-west-2/mleap/ml/data/data.csv\"\n",
    "## Update your bucket name and model location path ##\n",
    "s3_model_bucket='ruchika-wibd-west-2'\n",
    "spark_model_location='s3://'+s3_model_bucket+'/mleap/models/car_price_prediction_model/'\n",
    "pipeline_name='pipeline1'\n",
    "model_name='model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "data = spark.read.csv(path=input_file_path, header=True, quote='\"', sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Mileage: integer (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Trim: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Cylinder: integer (nullable = true)\n",
      " |-- Liter: double (nullable = true)\n",
      " |-- Doors: integer (nullable = true)\n",
      " |-- Cruise: integer (nullable = true)\n",
      " |-- Sound: integer (nullable = true)\n",
      " |-- Leather: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Trim': StringIndexer_4141a447f7f316da2b43, 'Make': StringIndexer_4838a136aa1639fbed00, 'Type': StringIndexer_45df90a97b35bfb67189, 'Model': StringIndexer_4ae79950dc676731930c}"
     ]
    }
   ],
   "source": [
    "data_test, data_train = data.randomSplit(weights=[0.3, 0.7], seed=10)\n",
    "\n",
    "get_indexer_input = get_indexer_input(data)\n",
    "print (get_indexer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(data_train, indexer_input):\n",
    "    x_cols = list(set(data_train.columns) - set(indexer_input.keys() + [\"Price\"]))\n",
    "    str_ind_cols = ['indexed_' + column for column in indexer_input.keys()]\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_tr = Pipeline(stages=indexers)\n",
    "    data_tr = pipeline_tr.fit(data_train).transform(data_train)\n",
    "    assembler = VectorAssembler(inputCols=x_cols, outputCol=\"features\")\n",
    "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Price\", stepSize=0.008, maxDepth=5, subsamplingRate=0.75,\n",
    "                       seed=10, maxIter=20, minInstancesPerNode=5, checkpointInterval=100, maxBins=64)\n",
    "    pipeline_training = Pipeline(stages=[assembler, gbt])\n",
    "    model = pipeline_training.fit(data_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    predictions = model.transform(data_te)\n",
    "    predictions.select(\"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_training(data_train, get_indexer_input)\n",
    "model.write().overwrite().save(spark_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "|Cruise|Cylinder|Doors|Leather|Liter|Make     |Mileage|Model|Price          |Sound|Trim        |Type |indexed_Trim|indexed_Make|indexed_Type|indexed_Model|\n",
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "|0     |4       |4    |1      |1.6  |Chevrolet|26191  |AVEO |9041.9062544231|0    |SVM Sedan 4D|Sedan|0.0         |0.0         |0.0         |0.0          |\n",
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "\n",
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|10236.175823272792|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "# Load the saved model and Test the model.\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml import Pipeline\n",
    "import json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    data_te.show(1,False)\n",
    "    predictions = model.transform(data_test)\n",
    "    predictions.select(\"prediction\").show(10,False)\n",
    "\n",
    "sameModel = PipelineModel.load(path=\"s3://ruchika-wibd-west-2/mleap/models/car_price_prediction_model/\")\n",
    "\n",
    "j={\"Price\":9041.9062544231,\"Mileage\":26191,\"Make\":\"Chevrolet\",\"Model\":\"AVEO\",\"Trim\":\"SVM Sedan 4D\",\"Type\":\"Sedan\",\"Cylinder\":4,\"Liter\"\n",
    ":1.6,\"Doors\":4,\"Cruise\":0,\"Sound\":0,\"Leather\":1}\n",
    "\n",
    "a=[json.dumps(j)]\n",
    "jsonRDD = sc.parallelize(a)\n",
    "df = spark.read.json(jsonRDD)\n",
    "\n",
    "get_indexer_input = get_indexer_input(df)\n",
    "model_testing(sameModel, df, get_indexer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+\n",
      "|          Price|Mileage|     Make|Model|        Trim| Type|Cylinder|Liter|Doors|Cruise|Sound|Leather|\n",
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+\n",
      "|9041.9062544231|  26191|Chevrolet| AVEO|SVM Sedan 4D|Sedan|       4|  1.6|    4|     0|    0|      1|\n",
      "+---------------+-------+---------+-----+------------+-----+--------+-----+-----+------+-----+-------+"
     ]
    }
   ],
   "source": [
    "row_df=data_test.limit(1)\n",
    "row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o503.serializeToBundle.\n",
      ": java.nio.file.NoSuchFileException: /tmp2/model.zip\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\n",
      "\tat java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:434)\n",
      "\tat java.nio.file.Files.newOutputStream(Files.java:216)\n",
      "\tat com.sun.nio.zipfs.ZipFileSystem.<init>(ZipFileSystem.java:116)\n",
      "\tat com.sun.nio.zipfs.ZipFileSystemProvider.newFileSystem(ZipFileSystemProvider.java:117)\n",
      "\tat java.nio.file.FileSystems.newFileSystem(FileSystems.java:326)\n",
      "\tat java.nio.file.FileSystems.newFileSystem(FileSystems.java:276)\n",
      "\tat ml.combust.bundle.BundleFile$.apply(BundleFile.scala:43)\n",
      "\tat ml.combust.bundle.BundleFile$.apply(BundleFile.scala:23)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$1.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$1.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat resource.DefaultManagedResource.open(AbstractManagedResource.scala:110)\n",
      "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:87)\n",
      "\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n",
      "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
      "\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/mleap/pyspark/spark_support.py\", line 25, in serializeToBundle\n",
      "    serializer.serializeToBundle(self, path, dataset=dataset)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/mleap/pyspark/spark_support.py\", line 42, in serializeToBundle\n",
      "    self._java_obj.serializeToBundle(transformer._to_java(), path, dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o503.serializeToBundle.\n",
      ": java.nio.file.NoSuchFileException: /tmp2/model.zip\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\n",
      "\tat java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:434)\n",
      "\tat java.nio.file.Files.newOutputStream(Files.java:216)\n",
      "\tat com.sun.nio.zipfs.ZipFileSystem.<init>(ZipFileSystem.java:116)\n",
      "\tat com.sun.nio.zipfs.ZipFileSystemProvider.newFileSystem(ZipFileSystemProvider.java:117)\n",
      "\tat java.nio.file.FileSystems.newFileSystem(FileSystems.java:326)\n",
      "\tat java.nio.file.FileSystems.newFileSystem(FileSystems.java:276)\n",
      "\tat ml.combust.bundle.BundleFile$.apply(BundleFile.scala:43)\n",
      "\tat ml.combust.bundle.BundleFile$.apply(BundleFile.scala:23)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$1.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$1.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat resource.DefaultManagedResource.open(AbstractManagedResource.scala:110)\n",
      "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:87)\n",
      "\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n",
      "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
      "\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Serialize to MLeap Bundle\n",
    "import mleap.pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "model.serializeToBundle(\"jar:file:/tmp/\"+model_name+\".zip\", model.transform(row_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Object(bucket_name='ruchika-wibd-west-2', key='mleap/models/pipeline1/model.zip')"
     ]
    }
   ],
   "source": [
    "#Save the Bundle to S3\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "data = open(\"/tmp/\"+model_name+\".zip\", 'rb')\n",
    "s3.Bucket(s3_model_bucket).put_object(Key='mleap/models/'+pipeline_name+'/'+model_name+'.zip', Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.zip\n",
      "cd /home/ec2-user/models/ ; tar -czvf model.tgz model.zip \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "## CHANGE ME ##\n",
    "s3_model_bucket='ruchika-wibd-west-2'\n",
    "spark_model_location='s3://'+s3_model_bucket+'/mleap/models/car_price_prediction_model/'\n",
    "pipeline_name='pipeline1'\n",
    "model_name='model'\n",
    "home='/home/ec2-user/models/'\n",
    "\n",
    "## Tar Zip the model and save back to S3\n",
    "import boto3,os\n",
    "\n",
    "## Check if local directory exists otherwise create it\n",
    "os.makedirs(home, exist_ok = True)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(s3_model_bucket).download_file('mleap/models/'+pipeline_name+'/'+model_name+'.zip', home+model_name+'.zip')\n",
    "cmd='cd '+home+' ; tar -czvf '+model_name+'.tgz '+model_name+'.zip '\n",
    "print (cmd)\n",
    "print (os.system(cmd))\n",
    "data = open(home+model_name+'.tgz', 'rb')\n",
    "s3.Bucket(s3_model_bucket).put_object(Key='mleap/models/'+pipeline_name+'/'+model_name+'.tgz', Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
